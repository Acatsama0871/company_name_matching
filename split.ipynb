{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import dask.dataframe as dd\n",
    "from typing import List, Dict\n",
    "from dask.diagnostics import ProgressBar\n",
    "ProgressBar().register()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# replace_dict = {'bank corporation': 'bank', 'bank corporate': 'bank', 'bank corp': 'bank'}\n",
    "\n",
    "# test_names = ['ORIX Bank Corporation', 'LLOYDS BANK CORPORATE MARKETS PLC', 'Trustco Bank Corp']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from typing import List, Dict\n",
    "\n",
    "# to_remove_words_list = [\n",
    "#     \"inc\",\n",
    "#     \"inc.\",\n",
    "#     \"incorporated\",\n",
    "#     \"llc\",\n",
    "#     \"l.l.c.\",\n",
    "#     \"l.l.c\",\n",
    "#     \"co.\",\n",
    "#     \"securities\",\n",
    "#     \"security\",\n",
    "#     \"s.a.\",\n",
    "#     \"s.a\",\n",
    "#     \"l.p.\",\n",
    "#     \"plc\",\n",
    "#     \"ag\",\n",
    "#     \"s.a.s\",\n",
    "#     \"s.a.s.\",\n",
    "#     \"s.p.a.\",\n",
    "#     \"s.p.a\",\n",
    "#     \"sa\",\n",
    "#     \"ab\",\n",
    "#     \"trust\",\n",
    "#     \"limited\",\n",
    "#     \"ltd\",\n",
    "#     \"ltd.\",\n",
    "# ]\n",
    "\n",
    "# replace_dict = {'bank corporation': 'bank', 'bank corporate': 'bank', 'bank corp': 'bank'}\n",
    "\n",
    "# def preprocess(cur_txt: str, to_remove_words_list: List=to_remove_words_list, replace_dict: Dict=replace_dict) -> str:\n",
    "#     \"\"\"\n",
    "#     Preprocess names before matching:\n",
    "#     1. remove commas\n",
    "#     2. replace by name\n",
    "#     3. remove words in to_remove_words_list\n",
    "\n",
    "#     Args:\n",
    "#         cur_txt (str): current name\n",
    "#         to_remove_words_list (List[srt]): list of words to remove\n",
    "\n",
    "#     Returns:\n",
    "#         str: preprocessed equity name\n",
    "#     \"\"\"\n",
    "#     #* 0. lower the name\n",
    "#     cur_txt = cur_txt.lower()\n",
    "#     #* 1. replace comma\n",
    "#     cur_txt = cur_txt.replace(\",\", \"\")\n",
    "#     #* 2. replace by name\n",
    "#     for cur_key in replace_dict:\n",
    "#         if cur_key in cur_txt:\n",
    "#             cur_txt = cur_txt.replace(cur_key, replace_dict[cur_key])\n",
    "#             break\n",
    "#     #* 3. remove words in to_remove_words_list\n",
    "#     cur_txt = cur_txt.split(\" \")\n",
    "#     result_str_list = [\n",
    "#         cur_word\n",
    "#         for cur_word in cur_txt\n",
    "#         if cur_word not in to_remove_words_list\n",
    "#     ]\n",
    "\n",
    "#     return \" \".join(result_str_list).strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preprocess(test_names[2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convert to csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Full Name</th>\n",
       "      <th>No.</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1 MAJ Metalska</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1 MAJ Zavrsni Rad Gradjevinarstvu</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1300 Smiles</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2 Park Street Trust</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>25 Novembar Celinac</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                           Full Name  No. \n",
       "0                     1 MAJ Metalska     1\n",
       "1  1 MAJ Zavrsni Rad Gradjevinarstvu     2\n",
       "2                        1300 Smiles     3\n",
       "3                2 Park Street Trust     4\n",
       "4                25 Novembar Celinac     5"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# search df\n",
    "search_df = pd.read_excel(os.path.join('data', '01_raw', 'search_df.xlsx'))\n",
    "search_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "search_df.to_csv(os.path.join('data', '02_primary', 'search.csv'), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Target Name</th>\n",
       "      <th>Target No.</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>360 Capital Industrial Fund</td>\n",
       "      <td>3543</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ABPA HOLDINGS LIMITED</td>\n",
       "      <td>9511</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>AIG International, Inc.</td>\n",
       "      <td>21931</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ALD International S.A.</td>\n",
       "      <td>8556</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>AmBev International Finance Co. Ltd.</td>\n",
       "      <td>4155</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                            Target Name  Target No.\n",
       "0           360 Capital Industrial Fund        3543\n",
       "1                 ABPA HOLDINGS LIMITED        9511\n",
       "2               AIG International, Inc.       21931\n",
       "3                ALD International S.A.        8556\n",
       "4  AmBev International Finance Co. Ltd.        4155"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# target df\n",
    "target_df = pd.read_excel(os.path.join('data', '01_raw', 'target_df.xlsx'))\n",
    "target_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_df.to_csv(os.path.join('data', '02_primary', 'target.csv'), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(588, 2)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Target Name', 'Target No.'], dtype='object')"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target_df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split Target Df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(588, 2)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><strong>Dask DataFrame Structure:</strong></div>\n",
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Target Name</th>\n",
       "      <th>Target No.</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>npartitions=30</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>object</td>\n",
       "      <td>int64</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>580</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>587</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>\n",
       "<div>Dask Name: from_pandas, 30 tasks</div>"
      ],
      "text/plain": [
       "Dask DataFrame Structure:\n",
       "               Target Name Target No.\n",
       "npartitions=30                       \n",
       "0                   object      int64\n",
       "20                     ...        ...\n",
       "...                    ...        ...\n",
       "580                    ...        ...\n",
       "587                    ...        ...\n",
       "Dask Name: from_pandas, 30 tasks"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target_df = pd.read_csv(\"data/02_primary/target.csv\")\n",
    "print(target_df.shape)\n",
    "target_df = dd.from_pandas(target_df, npartitions=30)\n",
    "target_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[########################################] | 100% Completed |  0.1s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['/research/haohang/text_matching/data/04_splits/00.csv',\n",
       " '/research/haohang/text_matching/data/04_splits/01.csv',\n",
       " '/research/haohang/text_matching/data/04_splits/02.csv',\n",
       " '/research/haohang/text_matching/data/04_splits/03.csv',\n",
       " '/research/haohang/text_matching/data/04_splits/04.csv',\n",
       " '/research/haohang/text_matching/data/04_splits/05.csv',\n",
       " '/research/haohang/text_matching/data/04_splits/06.csv',\n",
       " '/research/haohang/text_matching/data/04_splits/07.csv',\n",
       " '/research/haohang/text_matching/data/04_splits/08.csv',\n",
       " '/research/haohang/text_matching/data/04_splits/09.csv',\n",
       " '/research/haohang/text_matching/data/04_splits/10.csv',\n",
       " '/research/haohang/text_matching/data/04_splits/11.csv',\n",
       " '/research/haohang/text_matching/data/04_splits/12.csv',\n",
       " '/research/haohang/text_matching/data/04_splits/13.csv',\n",
       " '/research/haohang/text_matching/data/04_splits/14.csv',\n",
       " '/research/haohang/text_matching/data/04_splits/15.csv',\n",
       " '/research/haohang/text_matching/data/04_splits/16.csv',\n",
       " '/research/haohang/text_matching/data/04_splits/17.csv',\n",
       " '/research/haohang/text_matching/data/04_splits/18.csv',\n",
       " '/research/haohang/text_matching/data/04_splits/19.csv',\n",
       " '/research/haohang/text_matching/data/04_splits/20.csv',\n",
       " '/research/haohang/text_matching/data/04_splits/21.csv',\n",
       " '/research/haohang/text_matching/data/04_splits/22.csv',\n",
       " '/research/haohang/text_matching/data/04_splits/23.csv',\n",
       " '/research/haohang/text_matching/data/04_splits/24.csv',\n",
       " '/research/haohang/text_matching/data/04_splits/25.csv',\n",
       " '/research/haohang/text_matching/data/04_splits/26.csv',\n",
       " '/research/haohang/text_matching/data/04_splits/27.csv',\n",
       " '/research/haohang/text_matching/data/04_splits/28.csv',\n",
       " '/research/haohang/text_matching/data/04_splits/29.csv']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target_df.to_csv('data/04_splits/*.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocess Search df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(317751, 2)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "search_df = pd.read_csv(os.path.join('data', '02_primary', 'search.csv'))\n",
    "search_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "to_remove_words_list = [\n",
    "    \"inc\",\n",
    "    \"inc.\",\n",
    "    \"incorporated\",\n",
    "    \"llc\",\n",
    "    \"l.l.c.\",\n",
    "    \"l.l.c\",\n",
    "    \"co.\",\n",
    "    \"securities\",\n",
    "    \"security\",\n",
    "    \"s.a.\",\n",
    "    \"s.a\",\n",
    "    \"l.p.\",\n",
    "    \"plc\",\n",
    "    \"ag\",\n",
    "    \"s.a.s\",\n",
    "    \"s.a.s.\",\n",
    "    \"s.p.a.\",\n",
    "    \"s.p.a\",\n",
    "    \"sa\",\n",
    "    \"ab\",\n",
    "    \"trust\",\n",
    "    \"limited\",\n",
    "    \"ltd\",\n",
    "    \"ltd.\",\n",
    "]\n",
    "\n",
    "replace_dict = {'bank corporation': 'bank', 'bank corporate': 'bank', 'bank corp': 'bank'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(cur_txt: str, to_remove_words_list: List=to_remove_words_list, replace_dict: Dict=replace_dict) -> str:\n",
    "    \"\"\"\n",
    "    Preprocess names before matching:\n",
    "    1. remove commas\n",
    "    2. replace by name\n",
    "    3. remove words in to_remove_words_list\n",
    "\n",
    "    Args:\n",
    "        cur_txt (str): current name\n",
    "        to_remove_words_list (List[srt]): list of words to remove\n",
    "\n",
    "    Returns:\n",
    "        str: preprocessed equity name\n",
    "    \"\"\"\n",
    "    #* 0. lower the name\n",
    "    cur_txt = cur_txt.lower()\n",
    "    #* 1. replace comma\n",
    "    cur_txt = cur_txt.replace(\",\", \"\")\n",
    "    #* 2. replace by name\n",
    "    for cur_key in replace_dict:\n",
    "        if cur_key in cur_txt:\n",
    "            cur_txt = cur_txt.replace(cur_key, replace_dict[cur_key])\n",
    "            break\n",
    "    #* 3. remove words in to_remove_words_list\n",
    "    cur_txt = cur_txt.split(\" \")\n",
    "    result_str_list = [\n",
    "        cur_word\n",
    "        for cur_word in cur_txt\n",
    "        if cur_word not in to_remove_words_list\n",
    "    ]\n",
    "\n",
    "    return \" \".join(result_str_list).strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "search_df['preprocessed'] = search_df['Full Name'].apply(preprocess)\n",
    "with open(os.path.join('data', '03_feature', 'search_df.pkl'), 'wb') as f:\n",
    "    pickle.dump(search_df, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# search_df['preprocessed'] = search_df['Full Name'].apply(preprocess)\n",
    "# search_list = search_df['preprocessed'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# search_df_preprocessed2full_name = {cur_row['preprocessed']: cur_row['Full Name'] for _, cur_row in search_df.iterrows()}\n",
    "# search_df_full_name2id = {cur_row['Full Name']: cur_row['No. '] for _, cur_row in search_df.iterrows()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # save as pickle\n",
    "# with open(os.path.join('data', '03_feature', 'search_list.pkl'), 'wb') as f:\n",
    "#     pickle.dump(search_list, f)\n",
    "# with open(os.path.join('data', '03_feature', 'search_df_preprocessed2full_name.pkl'), 'wb') as f:\n",
    "#     pickle.dump(search_df_preprocessed2full_name, f)\n",
    "# with open(os.path.join('data', '03_feature', 'search_df_full_name2id.pkl'), 'wb') as f:\n",
    "#     pickle.dump(search_df_full_name2id, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('base_env')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "ea57f8ffba09633efa022ea1e2936fe77dafc7af9e2fff9c325ddf7d60cd6a04"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
